---
title: "R Final Exercise"
output: html_notebook
---

```{r}
install.packages('ggplot2',dependecies=True)
install.packages('missForest',dependecies=True)
library(missForest)
install.packages('e1071')
install.packages('caret')
install.packages('e1071')
library(caret)
library(ggplot2)
library(lattice)
library(e1071)   
install.packages("glmnet", repos = "http://cran.us.r-project.org")
library(glmnet)
install.packages("Hmisc")
library(Hmisc)
library(MASS)
```


## 1-.Carga los datos. Realiza una inspeccion por variables de la distribucion de aprobacion de credito en funcion de cada atributo visualmente. Realiza las observaciones pertinentes. ¿ Que variables son mejores para separar los datos?

Cargamos datos. Como el dataset contiene caracteres '?', hay que sustituirlos por 'NA' para posteriormente, imputar los 'NA'
```{r}
url <- "file:///D:/uSUARIOS/CASO_FINAL_crx.data"
df <- read.csv(url, header = FALSE,na.strings = c("?"))

#Realizamos un EDA
dim(df)
head(df)
str(df)

#Preparamos V16 con valores numÃ©ricos 
df$V1<-as.numeric(df$V1)-1
df$V9<-as.numeric(df$V9)-1
df$V10<-as.numeric(df$V10)-1
df$V12<-as.numeric(df$V12)-1
df$V16<-as.numeric(df$V16)-1
```
 Histogramas de las variables numÃ©ricas

```{r}
numeric_data <- as.data.frame(df[,c(2,3,8,11,14,15)])
par(mfrow=c(2,3))
for(i in 1:6) {
     hist(numeric_data[,i], main=names(numeric_data)[i])
}
```
 boxplots de cada variable respecto a la variable objetivo
```{r}
f<-ggplot(df,aes(V16,V1))
f+geom_count()
```

```{r}
f<-ggplot(df,aes(V16,V4))
f+geom_count()
```

```{r}
f<-ggplot(df,aes(V16,V5))
f+geom_count()
```

```{r}
f<-ggplot(df,aes(V16,V6))
f+geom_count()
```

```{r}
f<-ggplot(df,aes(V16,V7))
f+geom_count()
```

```{r}
f<-ggplot(df,aes(V16,V9))
f+geom_count()
```

```{r}
f<-ggplot(df,aes(V16,V10))
f+geom_count()
```

```{r}
f<-ggplot(df,aes(V16,V12))
f+geom_count()
```

```{r}
f<-ggplot(df,aes(V16,V13))
f+geom_count()
```

```{r}
f<-ggplot(df,aes(V16,V2))
f+geom_boxplot()
```

```{r}
f<-ggplot(df,aes(V16,V3))
f+geom_boxplot()
```

```{r}
f<-ggplot(df,aes(V16,V11))
f+geom_boxplot()
```

```{r}
f<-ggplot(df,aes(V16,V12))
f+geom_boxplot()
```

```{r}
f<-ggplot(df,aes(V16,V15))
f+geom_boxplot()
```

Como podemos comprobar, hay bastantes variables continuas que al ser representadas mediante boxplots, demuestran tener datos imprecisos que se encuentran muy lejanos a la mayoria de los datos de dicha variable.


```{r}
df$V11<- as.numeric(df$V11)
df$V14<- as.numeric(df$V14)
df$V15<- as.numeric(df$V15)

```




Las mejores variables para separar los datos son V1, V9, V10, V12, V16


2-.Prepara el dataset convenientemente e imputa los valores faltantes usando la libreria missForest

```{r}
#contamos el numero de  elementos vacios
sapply(df,function(x) sum(is.na(x)))

#Utilizamos missForest para imputar los valores faltantes y comprobamos finalmente si queda algun valor vacio
dataset<-missForest(df,maxiter = 10,ntree = 300)
dataset.imp<-dataset$ximp
sapply(dataset.imp,function(x) sum(is.na(x)))

str(dataset.imp)

attach(dataset.imp)

#dataset.imp$V16 <- factor(dataset.imp$V16,labels = c("0","1"))
```
#Matriz de correlacion de las variables numericas. Como vemos, no hay mucha relacion entre las variables. En todo caso, hay una relacion directa entre V8-V11 y entre V8-V2
```{r}
numeric_data_imp <- as.data.frame(df[,c(2,3,8,11,14,15)])
corrs <- rcorr(as.matrix(numeric_data_imp))
corrs$r
```

#Analizamos la independencia entre las variables categoricas y V16 mediante chi test.
Rechazamos la hipotesis nula tanto en V1 como con V12, rechazando asi la afirmacion de que son independientes con V16, ya que su p valor es menor de 0.05.
```{r}
tbl = table(dataset.imp$V1, dataset.imp$V16)
chisq.test(tbl)

tbl = table(dataset.imp$V4, dataset.imp$V16)
chisq.test(tbl)

tbl = table(dataset.imp$V5, dataset.imp$V16)
chisq.test(tbl)

tbl = table(dataset.imp$V6, dataset.imp$V16)
chisq.test(tbl)

tbl = table(dataset.imp$V7, dataset.imp$V16)
chisq.test(tbl)

tbl = table(dataset.imp$V9, dataset.imp$V16)
chisq.test(tbl)

tbl = table(dataset.imp$V10, dataset.imp$V16)
chisq.test(tbl)

tbl = table(dataset.imp$V12, dataset.imp$V16)
chisq.test(tbl)

tbl = table(dataset.imp$V13, dataset.imp$V16)
chisq.test(tbl)
```

#Seleccion del mejor modelo en funcion de los atributos usando el criterio AIC, de forma que aumenten el numero de variables progresivamente.
```{r}
full<-glm(V16~.,data=dataset.imp)
summary(full)
```
Hemos visto que tanto V9 como V10 V13p y V6x están fuertemente relacionadas con V16


```{r}
step<-stepAIC(full,direction = "backward")
```
La mejor opcion es la de un AIC mayor, es decir el model con todas las variables exceptuando la V5. Se tienen AICs optimos con las variables fuertes vistas anteriormente pero finalmente, el modelo con todas las variables menos la V5 es el mejor. 



##3-.Divide el dataset tomando las primeras 590 instancias como train y las ultimas 100 como test.
```{r}
dataset_train <- dataset.imp[1:590, 1:16]
dataset_test <- dataset.imp[591:690, 1:16]
x <- dataset.imp[,1:15]
y <- dataset.imp$V16

x_train<-x[1:590,]
y_train<-y[1:590]
x_test<-x[591:690,]
y_test<-(y[591:690])

X_train<-data.matrix(x_train)
x_test<-data.matrix(x_test)

```
4-.Entrena un modelo de regresiÃ³n logÃ­stica con regularizaciÃ³n Ridge y Lasso en train seleccionando el que mejor AUC tenga. Da las mÃ©tricas en test.

------------------------RIDGE--------------------------------
```{r}
##model.log<- glm(V16~., data=dataset_train, family=binomial)


set.seed(999)
rl.ridge <- cv.glmnet(X_train, y_train, family='binomial', alpha=0, parallel=TRUE, standardize=TRUE, type.measure='auc')
```
   RepresentaciÃ³n, lamda min y error, coeficientes
```{r}
plot(rl.ridge)
rl.ridge$lamda.min
max(rl.ridge$cvm)
coef(rl.ridge, s=rl.ridge$lambda.min)
```
    MÃ©tricas
```{r}
require(methods)
y_pred <- as.numeric(predict.glmnet(rl.ridge$glmnet.fit, newx=x_test, s=rl.ridge$lambda.min)>.5)

confusionMatrix(as.factor(y_test), as.factor(y_pred), mode="everything")
```

------------------------LASO--------------------------------
```{r}
set.seed(999)
rl.lasso <- cv.glmnet(X_train, y_train, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='auc')
```
    RepresentaciÃ³n, lamda min y error, coeficientes
```{r}
plot(rl.lasso)
rl.lasso$lamda.min
max(rl.lasso$cvm)
coef(rl.lasso, s=rl.lasso$lambda.min)
```
   MÃ©tricas
```{r}
require(methods)
y_pred <- as.numeric(predict.glmnet(rl.lasso$glmnet.fit, newx=x_test, s=rl.lasso$lambda.min)>.5)
confusionMatrix(as.factor(y_test), as.factor(y_pred), mode="everything")
```

Definitivamente el modelo que mayor area engloba debajo de la curva es el model de Ridge. Sin embargo ambos tienen la misma Accuracy  y el modelo Lasso tiene mejor Sensibilidad.


5-.Aporta los log odds de las variables predictoras sobre la variable objetivo.
```{r}
dataset.imp$V16 <- factor(dataset.imp$V16,labels = c("0","1"))
dataset.imp$V1 <- factor(dataset.imp$V1,labels = c("0","1"))
dataset.imp$V9 <- factor(dataset.imp$V9,labels = c("0","1"))
dataset.imp$V10 <- factor(dataset.imp$V10,labels = c("0","1"))
dataset.imp$V12 <- factor(dataset.imp$V12,labels = c("0","1"))

hist(dt$V16)

mod <- full
mod$coefficients
```
-Vemos que el cambio en la Variable V9t aumentarÃ¡ las oportunidad de tener un crÃ©dito aprovado con gran diferencia.

-La desviaciÃ³n nula  es considerablemente mayor que la DesviaciÃ³n de los residuos por lo que podemos comprobar que el modelo se comporta mucho mejor con variables predictores que solo con el intercept

-Vemos ahora con Anova como son las desviaciones de las pariables predictoras. Se puede comprobar que a medida que vamos introduciendo variables en el modelo ( de arriba a abajo) la desviaciÃ³n se reduce y por lo tanto el model se comporta mejor.

```{r}
anova(mod,test="Chisq")
```

-Analizamos los log odds 
```{r}
options(scipen=999) 
coef(mod)
exp(coef(mod))
```
Se puede observar que el factor que más influye es la V9, que hace que la probabilidad de obtener un credito aprobado sea 1.78 veces la de una persona que no tiene V9. 

Tener V13p provoca que obtener el aprobado sea 1.6 veces mas facil.



6-.Si por cada verdadero positivo ganamos 100e y por cada falso positivo perdemos 20e. Â¿ QuÃ© rentabilidad aporta aplicar este modelo?
Con Ridge: 86 VP y 0 FN
Con Lasso: 85 VP y 1 FN

El que mayor rentabilidad aporta seria Ridge, del 100% , 98%.


